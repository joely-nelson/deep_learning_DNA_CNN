{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fn1DdIWOJybv"
   },
   "source": [
    "# DNA Sequence Classification by CNNs\n",
    "Deep Learning Project. Dec 14, 2020\n",
    "By Joely Nelson\n",
    "\n",
    "In this project, I developed a convolutional neural network to classify DNA sequences from two-data sets. I mimic the architecture of the CNN used in prior work on two different datasets, and achieve close to their accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nTjUV1mjum9r"
   },
   "source": [
    "# Intro\n",
    "DNA carries genetic instructions for the development, function, growth and reproduction of all known organisms. DNA is essentially the programming language of life; information stored as A, T, G, and C. It makes sense that we should be able to use DNA sequences to make predictions about a gene’s function, classification, and transcription levels. However, this is extremely difficult, as there are many unknown features, and many bioinformatics algorithms must be generated by hand.\n",
    "\n",
    "With the emergence of deep learning, which makes use of automatic feature learning, and the increase of available genetic data sets, there is an opportunity to apply deep learning to genetic datasets to make complex predictions. Machine learning models and deep neural networks have been used to predict the non-coding function of DNA, identifying DNA-binding proteins, and even predicting the transcription level of genes. This is still a new field and there is much more to be explored.\n",
    "\n",
    "In this project, I tried to reproduce some of the work done in the paper DNA Sequence Classification by Convolutional Neural Network. Researchers treated DNA data like text and used CNNs to classify various DNA sequences. By reproducing their architecture, I am able to fine tune models that achieve close to their accuracy or better.\n",
    "\n",
    "I wanted to do this project because I am currently applying data-driven approaches to learn more about the rules of CRISPR in the Carothers Research Group at the university of Washington. I am planning on implementing a network to classify and generate sequences that are good for CRISPR. I would like to better my understanding of making models with sequence data so when data for my project is collected, I’m ready to hit the ground running.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zyBZ65oGuXiH"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEZCw26lsmBo"
   },
   "source": [
    "## Imports\n",
    "\n",
    "Here are the packages we import to work on this project as well as any other general setup that needs to occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ZNbuEfDrz0Y"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import sys\n",
    "import pandas as pd\n",
    "import re\n",
    "from torch.utils.data import (TensorDataset, DataLoader, RandomSampler,\n",
    "                              SequentialSampler)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "# from Bio import SeqIO \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pASQ98Frz0b"
   },
   "source": [
    "## Preprocessing the data\n",
    "DNA data is usually represented in strings made up of the characters A, T, G, and C. We can't train our network with just these strings and will need to do some kind of preprocessing of our data before we can feed it into our neural network.\n",
    "\n",
    "<br>\n",
    "\n",
    "### DNA As Strings\n",
    "In the paper DNA Sequence Classification by Convolutional Neural Network, researchers treated DNA sequences like string tokens in language. Using this idea, we will attempt to convert DNA sequences into tokenized sequences.\n",
    "\n",
    "We require a dictionary that maps the various tokens in the sequence (ie ATCG) to a numerical token. \n",
    "\n",
    "<br>\n",
    "\n",
    "Here is an example of a mapping from sequence letters to integer tokens:\n",
    "\n",
    "``{'A': 2, 'T': 3, 'C': 4, 'G':5}``\n",
    "\n",
    "Using this map we can convert DNA sequences into tokenized sequences. For example this sequence:\n",
    "\n",
    "``\"GATCGTAGTCCTAGTAAGACTGAC\"``\n",
    "\n",
    "Would be tokenized as this:\n",
    "\n",
    "``[5, 2, 3, 4, 5, 3, 2, 5, 3, 4, 4, 3, 2, 5, 3, 2, 2, 5, 2, 4, 3, 5, 2, 4]``\n",
    "\n",
    "<br>\n",
    "\n",
    "### Variable Sequence Lengths and Unknown Tokens\n",
    "It is possible for some of the datasets that we will be analyzing, the various input sequences may be of different length. Unlike images or text, it ,may be unfeasible to resize or crop sequences. Instead we will add on an additional token, ``<pad>`` will go onto the tail end of any sequence that is too short.\n",
    "\n",
    "In the future, someone else may want to use the dataset. It may also be possible that unknown tokens are found in that dataset, so we add on an ``<unk>`` token to represent unknown characters.\n",
    "\n",
    "<br>\n",
    "\n",
    "For example, if we had a dataset that consisted of the following two sequences:\n",
    "\n",
    "'GATCGTAGTCCTAGTAAGACTGAC', 'TAGCC'\n",
    "\n",
    "Then we might see a vocabulary to token mapping like this:\n",
    "\n",
    "``{'<pad>': 0, '<unk>': 1, 'A': 2, 'T': 3, 'C': 4, 'G': 5}``\n",
    "\n",
    "When tokenized and padded, the two sequences will look like this, respectivley. Note the long train of 0s on the smaller sequence to represent the padding.\n",
    "\n",
    "``[5., 2., 3., 4., 5., 3., 2., 5., 3., 4., 4., 3., 2., 5., 3., 2., 2., 5., 2., 4., 3., 5., 2., 4.]``\n",
    "\n",
    "``[3., 2., 5., 4., 4., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]``\n",
    "\n",
    "\n",
    "<br>\n",
    "SOURCES:\n",
    "We are going to use similiar thinking to sentence construction.\n",
    "https://chriskhanhtran.github.io/posts/cnn-sentence-classification/\n",
    "\n",
    "Data from: https://europepmc.org/article/med/16122420"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2i177xGFrz0b",
    "outputId": "d770b690-8617-4312-d6a4-cb456275d38a"
   },
   "outputs": [],
   "source": [
    "def label_to_count(labels):\n",
    "    '''\n",
    "    Given a list of labels, returns a dictionary that maps each class label to how many\n",
    "    instances of that label were present in the list.\n",
    "    '''\n",
    "    label_to_count_dict = {}\n",
    "    for label in labels:\n",
    "        if label not in label_to_count_dict:\n",
    "            label_to_count_dict[label] = 0\n",
    "        label_to_count_dict[label] += 1\n",
    "    return label_to_count_dict\n",
    "\n",
    "\n",
    "def prepare_data(seqs):\n",
    "    '''\n",
    "    Given a list of sequences, will turn into a tokenized vector.\n",
    "    \n",
    "    ARGS:\n",
    "        seqs: a list of strings where every string is a sequence\n",
    "    RETURNS:\n",
    "        tokenized_seqs (list(list(int))): list of list of tokens\n",
    "        voc2ind (dict) a dictionary where keys are letters, values are the corresponding token\n",
    "    '''\n",
    "    max_len = 0\n",
    "    \n",
    "    # build up a voc2ind (letters:token)\n",
    "    # based on ATGC and include padding and unknown tokens\n",
    "    voc2ind = {voc:ind for ind,voc in enumerate(['<pad>', '<unk>', 'A', 'T', 'C', 'G'])}\n",
    "    \n",
    "    i = len(voc2ind)\n",
    "    \n",
    "    # tokenize the sequences\n",
    "    tokenized_seqs = []\n",
    "    for seq in seqs:\n",
    "        tokenized_seq = []\n",
    "        for e in seq:\n",
    "            # make sure the sequence is upper case, a == A\n",
    "            seq = seq.upper()\n",
    "            # if we haven't seen this letter before, add to the corupus\n",
    "            if not e in voc2ind:\n",
    "                voc2ind[e] = i\n",
    "                i += 1\n",
    "            tokenized_seq.append(voc2ind[e])\n",
    "        tokenized_seqs.append(tokenized_seq)\n",
    "        \n",
    "    return tokenized_seqs, voc2ind\n",
    "        \n",
    "res = prepare_data(['ATCG', 'TAGA', 'APO'])\n",
    "print(res)\n",
    "assert(res[0] == [[2, 3, 4, 5], [3, 2, 5, 2], [2, 6, 7]]), res[0]\n",
    "\n",
    "\n",
    "def prepare_labels(labels):\n",
    "    '''\n",
    "    Given a list of labels will turn them into integer labels\n",
    "    Args:\n",
    "        labels: a list of labels\n",
    "    Returns:\n",
    "        tokenized_labels: numpy array(list) a list of label tokens\n",
    "        label2token: (dict) a dictionary where keys are letters, values are corresponding token\n",
    "    '''\n",
    "    tokenized_labels = []\n",
    "    label2token = {}\n",
    "    i = 0\n",
    "    for label in labels:\n",
    "        if not label in label2token:\n",
    "            label2token[label] = i\n",
    "            i += 1\n",
    "        tokenized_labels.append(label2token[label])\n",
    "    return tokenized_labels, label2token\n",
    "\n",
    "\n",
    "def pad(tokenized_seqs, voc2ind):\n",
    "    '''\n",
    "    Pad each sequence to the maximum length by adding a <pad> token\n",
    "    \n",
    "    ARGS:\n",
    "        tokenized_seqs (list(list(str))): list of list of tokens\n",
    "        voc2ind (dict) a dictionary where keys are letters, values are the corresponding token\n",
    "    RETURNS:\n",
    "        a numpy array of all the tokenized sequences that have been padded to be the same\n",
    "        length.\n",
    "    '''\n",
    "\n",
    "    padded_seqs = []\n",
    "    \n",
    "    # find max sequence length\n",
    "    max_len = 0\n",
    "    for seq in tokenized_seqs:\n",
    "        max_len = max(len(seq), max_len)\n",
    "    \n",
    "    # add padding so sequences are max_length\n",
    "    for seq in tokenized_seqs:\n",
    "        padded_seq = seq + [voc2ind['<pad>']] * (max_len - len(seq))\n",
    "        padded_seqs.append(padded_seq)\n",
    "        \n",
    "    return np.array(padded_seqs, dtype=np.float32)\n",
    "\n",
    "\n",
    "def data_loader(train_inputs, val_inputs, train_labels, val_labels,\n",
    "                batch_size=50):\n",
    "    \"\"\"\n",
    "    Convert train and validation sets to torch.Tensors and load them to\n",
    "    DataLoader.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert data type to torch.Tensor\n",
    "    train_inputs, val_inputs, train_labels, val_labels =\\\n",
    "    tuple(torch.tensor(data) for data in\n",
    "          [train_inputs, val_inputs, train_labels, val_labels])\n",
    "\n",
    "    # Create DataLoader for training data\n",
    "    train_data = TensorDataset(train_inputs, train_labels)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "    # Create DataLoader for validation data\n",
    "    val_data = TensorDataset(val_inputs, val_labels)\n",
    "    val_sampler = SequentialSampler(val_data)\n",
    "    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AonQgFcPsHA7"
   },
   "source": [
    "## Training\n",
    "Below we have a generic training function which will train a given net. We also have an accuracy evaluation function for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AD0IQU5LLp1m"
   },
   "outputs": [],
   "source": [
    "def train(net, dataloader, epochs=1, lr=0.01, momentum=0.9, decay=0.0, verbose=1):\n",
    "  ''' Trains a neural network. Returns a 2d numpy array, where every list \n",
    "  represents the losses per epoch.\n",
    "  '''\n",
    "  net.to(device)\n",
    "  losses_per_epoch = []\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum, weight_decay=decay)\n",
    "  for epoch in range(epochs):\n",
    "    sum_loss = 0.0\n",
    "    losses = []\n",
    "    for i, batch in enumerate(dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize \n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        losses.append(loss.item())\n",
    "        sum_loss += loss.item()\n",
    "        if i % 100 == 99:    # print every 100 mini-batches\n",
    "            if verbose:\n",
    "              print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, sum_loss / 100))\n",
    "            sum_loss = 0.0\n",
    "    # print(len(losses))\n",
    "    losses_per_epoch.append(np.mean(losses))\n",
    "  return losses_per_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XB4V1CTHLEFf"
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Below are a few functions used for evaulating the function such as accuracy, printing the accuracy scores for a trained model, and plotting the loss over epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0hY5-JvaLJUi"
   },
   "outputs": [],
   "source": [
    "def accuracy(net, dataloader):\n",
    "    '''\n",
    "    Given a trained neural network and a dataloader, computes the accuracy.\n",
    "    Arguments:\n",
    "        net: a neural network\n",
    "        dataloader: a dataloader\n",
    "    Returns:\n",
    "        fraction of examples classified correctly (float)\n",
    "        number of correct examples (int)\n",
    "        number of total examples (float)\n",
    "    '''\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input, labels = batch[0].to(device), batch[1].to(device)\n",
    "            outputs = net(input)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct/total, correct, total\n",
    "\n",
    "def print_eval(net, train_dataloader, test_dataloader):\n",
    "    '''\n",
    "    Given a test and train data loader, prints the test and train accuracy and\n",
    "    the number of examples they got right.\n",
    "    RETURNS\n",
    "        (train_acc, test_acc) results of running accuracy on the two dataloaders\n",
    "    '''\n",
    "    train_acc = accuracy(net, train_dataloader)\n",
    "    test_acc = accuracy(net, test_dataloader)\n",
    "    \n",
    "\n",
    "    print(\"Train accuracy: \" + str(train_acc[0]) + \"\\t(\" + str(train_acc[1]) + \"/\" + str(train_acc[2]) + \")\")\n",
    "    print(\"Test accuracy: \" + str(test_acc[0]) + \"\\t(\" + str(test_acc[1]) + \"/\" + str(test_acc[2]) + \")\")\n",
    "          \n",
    "    return train_acc, test_acc\n",
    "\n",
    "def plot_losses(losses, smooth_val = None, title = \"\"):\n",
    "    '''\n",
    "    Plots the losses per epoch returned by the training function.\n",
    "    Args:\n",
    "        losses: a list of losses returned by train\n",
    "        smooth_val: an optinal integer value if smoothing is desired\n",
    "        title: a title for the graph\n",
    "    '''\n",
    "    # loss = np.mean(losses, axis = 1)\n",
    "    epochs = [i for i in range(1, len(losses) + 1)]\n",
    "    if smooth_val is not None:\n",
    "        lossses = smooth(losses, smooth_val)\n",
    "    plt.plot(epochs, losses, marker=\"o\", linestyle=\"dashed\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(title)\n",
    "    \n",
    "\n",
    "def smooth(x, size):\n",
    "    '''\n",
    "    Given an array, smooths it by some number size, to make it look less janky.\n",
    "    '''\n",
    "    return np.convolve(x, np.ones(size)/size, mode='same')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a5sPU7nCrz0c"
   },
   "source": [
    "# Splice Dataset\n",
    "https://archive.ics.uci.edu/ml/datasets/Molecular+Biology+(Splice-junction+Gene+Sequences)\n",
    "\n",
    "\n",
    "This dataset contains information about splice junctions. In genes, regions which are removed during the RNA transcription process are called introns, and regions that are used to generate mRNA are called exons. Junctions between them are called splice junctions. There are two kinds of splice junction that is exon-intron junction, and intron-exon junctions.\n",
    "\n",
    "Each of the sequences in this dataset are 60bp long, and belong to one of 3 classes: \"EI\" (Extron-Intron junction), \"IE\" (Intron-Extron junction) and \"N\" (neither EI or IE).\n",
    "\n",
    "There are 767 genes with the EI label, 768 with the IE label, and 1655 with the N label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QOmk0TIvrz0c",
    "outputId": "6263e272-bffe-407c-9897-4e9a5d071b78"
   },
   "outputs": [],
   "source": [
    "# loading dataset\n",
    "# all sequences put in seq\n",
    "# all labels in corresponding index in labels\n",
    "seqs = []\n",
    "labels = []\n",
    "\n",
    "f = open('splice.data')\n",
    "for line in f:\n",
    "    line = [i.replace(',', '') for i in line.split()]\n",
    "    seqs.append(line[2])\n",
    "    labels.append(line[0])\n",
    "f.close()\n",
    "\n",
    "print(seqs)\n",
    "\n",
    "# tokenizing and getting a vocab\n",
    "tokenized_seqs, voc2ind = prepare_data(seqs)\n",
    "\n",
    "# padding\n",
    "tokenized_seqs = pad(tokenized_seqs, voc2ind)\n",
    "\n",
    "# tokenizing labels\n",
    "tokenized_labels, label2token = prepare_labels(labels)\n",
    "\n",
    "# Showing the result of this:\n",
    "print(\"\\n\", tokenized_seqs, \"\\n\\n\", voc2ind, \"\\n\\n\", label_to_count(labels))\n",
    "\n",
    "# Train Test Split\n",
    "train_inputs, test_inputs, train_labels, test_labels = train_test_split(\n",
    "    tokenized_seqs, tokenized_labels, test_size=0.1, random_state=42)\n",
    "\n",
    "# Load data to PyTorch DataLoader\n",
    "train_dataloader, test_dataloader = data_loader(train_inputs, test_inputs, train_labels, test_labels, batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qsq48UxewN7A"
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TzewhfOYz1OE"
   },
   "source": [
    "### Simple Model\n",
    "This extremly simple neural network is to sanity check that the data has been processed correctly. Also if an extremly simple neural network with just a relu will classify output, that would also be great."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "0ki7_-BPLJx6",
    "outputId": "2ba4b060-ee6c-4193-b6d3-e25635d0a219"
   },
   "outputs": [],
   "source": [
    "FEATURE_SIZE = tokenized_seqs.shape[1]\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, vocab_size, feature_size, num_classes=3):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        # UM?\n",
    "        self.vocab_size = vocab_size\n",
    "        # embeded_dim: Dimension of word vectors.\n",
    "        self.feature_size = feature_size\n",
    "\n",
    "        self.encoder = nn.Embedding(self.vocab_size, self.feature_size)\n",
    "\n",
    "        self.fc1 = nn.Linear(feature_size, 1000)\n",
    "        self.fc2 = nn.Linear(1000, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Creating net and training\n",
    "net = SimpleNet(len(voc2ind), FEATURE_SIZE)\n",
    "losses = train(net, train_dataloader, epochs = 10)\n",
    "\n",
    "# Evaluation\n",
    "plot_losses(losses, title = \"Splice Dataset: Convolutional Network Loss\")\n",
    "print_eval(net, train_dataloader, test_dataloader)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LhcbTv_T4gGg"
   },
   "source": [
    "This model seems to classify quite alright, even with such a simple network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-TOexQx2bH4"
   },
   "source": [
    "### Convolutional Model with 2D Max Pooling\n",
    "We will now attempt to implement the model from the [DNA Sequence Classification by Convolutional Neural Network paper](https://www.researchgate.net/publication/301703031_DNA_Sequence_Classification_by_Convolutional_Neural_Network) Here is the discussion that they leave:\n",
    "\n",
    "\"*The model contains 2 convolutional layers. Each of these layers is followed by a sub-sampling layer. These layers are used to extract features from representation matrixes  of  sequences.  The  extracted  features  are  then  transformed  by  using  a  fully  connected  neural  network  layer which contains 100 neurons. In this layer, we used a dropout value of 0.5 to reduce the effect overfitting. Finally, a soft max output layer is used to predict labels of input sequences.*\"\n",
    "\n",
    "This is not a lot to go off of. The researchers do not mention an activation function, but we will use one because otherwise this model is no better than a linear one. We will attempt to follow their architecture as closely as possible and attempt to get their accuracy score of 96%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7oja0rIpE36r",
    "outputId": "5f1ee6bb-9bce-4cda-a7b8-a1b961938a37"
   },
   "outputs": [],
   "source": [
    "480/60 * 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "id": "v0vcIk8hsK0W",
    "outputId": "a2a7d59f-2e7e-4ca0-f16a-9cdfef82d6c2"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Here are some experiments run with the model\n",
    "# kind of represents a table, each row is an experiment.\n",
    "# the first row is the indecies. Each row should be the same length.\n",
    "# test acc and train acc recorded for 10 epochs, are not neccesarily the final model accuracy reported in my report\n",
    "ex = [['in_channels_conv1', 'out_channels_conv1', 'in_channels_conv2', 'out_channels_conv2', 'in_channels_fc1', 'kernel_size', 'stride', ('train_acc'), ('test_acc')],\n",
    "      [60, 40, 20, 40, 40, 3, 3], #1\n",
    "      [60, 240, 120, 480, 480, 3, 3], #2 \n",
    "      [60, 480, 240, 960, 960, 3, 3, 0, (0.973528), (0.918495)], #3\n",
    "      [60, 480, 240, 960, 960, 6, 3, 0, (0.998607), (0.962382)], #4\n",
    "      [60, 480, 240, 960, 960, 6, 3,  0.01, (0.991988), (0.9717)], #5 BEST ONE! :)\n",
    "      [10, 80, 40, 160, 160, 6, 3,  0.01, (0.9738766980146291), (0.9561128526645768)], #6\n",
    "      [60, 480, 240, 960, 960, 6, 3,  0.02, (0.9641239986067572), (0.934169278996865)], #7\n",
    "]\n",
    "i = 5\n",
    "\n",
    "FEATURE_SIZE = ex[i][0]\n",
    "\n",
    "class DSC_by_CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, feature_size, num_classes=3):\n",
    "\n",
    "\n",
    "        super(DSC_by_CNN, self).__init__()\n",
    "        # UM?\n",
    "        self.vocab_size = vocab_size\n",
    "        # embeded_dim: Dimension of word vectors.\n",
    "        self.feature_size = feature_size\n",
    "\n",
    "        self.encoder = nn.Embedding(self.vocab_size, self.feature_size)\n",
    "        \n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=ex[i][0],\n",
    "                               out_channels=ex[i][1], \n",
    "                               kernel_size=ex[i][5],\n",
    "                               stride = ex[i][6],\n",
    "                               padding = 1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=ex[i][2],\n",
    "                               out_channels=ex[i][3], \n",
    "                               kernel_size=ex[i][5],\n",
    "                               stride = ex[i][6],\n",
    "                               padding = 1)\n",
    "\n",
    "\n",
    "        # Activation\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "        # Pooling\n",
    "        self.pool = nn.MaxPool2d(3, 2, 1)\n",
    "\n",
    "        # fully connected layer\n",
    "        self.fc1 = nn.Linear(ex[i][4], 100)\n",
    "\n",
    "        # dropout\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "\n",
    "\n",
    "    # seen in the network pytorch tutorial\n",
    "    # I've been using this function for years, no idea what it does.\n",
    "    # Would it kill the pytorch tutorial people to document this guy?\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The following line is required to send it through the encoder. Not sure why.\n",
    "        # b_input_ids = torch.tensor(b_input_ids).to(device).long()\n",
    "        x = torch.tensor(x).to(torch.int64)\n",
    "\n",
    "        # Encode. \n",
    "        # Output shape: (b, max_len, embedded_dim)\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # Permute shuffles dimensions. Do this to satisfy requirments of nn.Conv1d\n",
    "        # Output shape: (b, embedded_dim, max_len)\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        # convolutional layer 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # convolutional layer 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # flattening\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "\n",
    "        # Tring to figure out the shape\n",
    "        #print(x.shape)\n",
    "\n",
    "        # fully linear layer + dropout\n",
    "        x = self.drop(x)\n",
    "        # x = self.softm(x)\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Creating net and training\n",
    "net = DSC_by_CNN(len(voc2ind), FEATURE_SIZE)\n",
    "losses = train(net, train_dataloader, epochs=50, lr=0.01, decay = ex[i][-3])\n",
    "\n",
    "# Evaluation\n",
    "plot_losses(losses, title = \"Splice Dataset: Convolutional Network Loss\")\n",
    "print_eval(net, train_dataloader, test_dataloader)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iw6sdN7aIPmO"
   },
   "source": [
    "### Convolutional Model with 1D Max Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "id": "84U884sSD8z7",
    "outputId": "3fb5e918-3e8e-4a6d-f067-dee86538896e"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Here are some experiments run with the model\n",
    "# kind of represents a table, each row is an experiment.\n",
    "# the first row is the indecies. Each row should be the same length.\n",
    "ex = [['in_channels_conv1', 'out_channels_conv1', 'in_channels_conv2', 'out_channels_conv2', 'in_channels_fc1', \n",
    "       'kernel_size_conv1', 'stride_conv1', 'kenrnel_size_conv2', 'kernel_size_conv1', \n",
    "       'momentum', 'learning rate', 'decay', ('train_acc'), ('test_acc'),],\n",
    "      [60, 480, 480, 960, 1920, 6, 3, 6, 3,   0.8,0.01, 0.02, (0.8193495693495694), (0.7989311957247829)], #1\n",
    "]\n",
    "i = 1\n",
    "\n",
    "FEATURE_SIZE = ex[i][0]\n",
    "\n",
    "class H3_DSC_by_CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, feature_size, num_classes=2):\n",
    "\n",
    "\n",
    "        super(H3_DSC_by_CNN, self).__init__()\n",
    "        # UM?\n",
    "        self.vocab_size = vocab_size\n",
    "        # embeded_dim: Dimension of word vectors.\n",
    "        self.feature_size = feature_size\n",
    "\n",
    "        self.encoder = nn.Embedding(self.vocab_size, self.feature_size)\n",
    "        \n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=ex[i][0],\n",
    "                               out_channels=ex[i][1], \n",
    "                               kernel_size=ex[i][5],\n",
    "                               stride = ex[i][6],\n",
    "                               padding = 1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=ex[i][2],\n",
    "                               out_channels=ex[i][3], \n",
    "                               kernel_size=ex[i][7],\n",
    "                               stride = ex[i][8],\n",
    "                               padding = 1)\n",
    "\n",
    "\n",
    "        # Activation\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "        # Pooling\n",
    "        self.pool = nn.MaxPool1d(3, 2, 1)\n",
    "\n",
    "        # fully connected layer\n",
    "        self.fc1 = nn.Linear(ex[i][4], 100)\n",
    "\n",
    "        # dropout\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "\n",
    "        # softmax\n",
    "        self.softm = nn.Softmax()\n",
    "\n",
    "\n",
    "    # seen in the network pytorch tutorial\n",
    "    # I've been using this function for years, no idea what it does.\n",
    "    # Would it kill the pytorch tutorial people to document this guy?\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The following line is required to send it through the encoder. Not sure why.\n",
    "        # b_input_ids = torch.tensor(b_input_ids).to(device).long()\n",
    "        x = torch.tensor(x).to(torch.int64)\n",
    "\n",
    "        # Encode. \n",
    "        # Output shape: (b, max_len, embedded_dim)\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # idk flatten it?\n",
    "        # x = torch.flatten(x, 1)\n",
    "\n",
    "        # Permute shuffles dimensions. Do this to satisfy requirments of nn.Conv1d\n",
    "        # Output shape: (b, embedded_dim, max_len)\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        # convolutional layer 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # convolutional layer 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # flattening?\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "\n",
    "        # Tring to figure out the shape\n",
    "        # print(x.shape)\n",
    "\n",
    "        # fully linear layer + dropout\n",
    "        x = self.drop(x)\n",
    "        # x = self.softm(x)\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Creating net and training\n",
    "net = H3_DSC_by_CNN(len(voc2ind), FEATURE_SIZE)\n",
    "losses = train(net, train_dataloader, epochs = 20, decay = ex[i][-3], lr=ex[i][-4], momentum=ex[i][-5])\n",
    "\n",
    "# Evaluation\n",
    "plot_losses(losses, title = \"Splice Dataset: Convolutional Network Loss\")\n",
    "print_eval(net, train_dataloader, test_dataloader)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BNfoIjr3ZOPN"
   },
   "source": [
    "# H3 Dataset\n",
    "http://www.jaist.ac.jp/~tran/nucleosome/members.htm \n",
    "\n",
    "These datasets are about DNA sequences wrapping around histone proteins. It is  mechanism used to pack and store long DNA sequence into a cell’s nucleus. Name convention in these datasets is constructed as follows: “H3” or “H4” indicates the histone type, “K” and a succeeding number indicate a modified amino acid (e.g. “K14” denotes the 14th amino acid “K” has been modified), and “ac” or “me” indicate the type of modification (acetylation or methylation) and a num-\n",
    "ber after “me” indicates times of methylation. In each dataset, samples are sequences with length of 500 base pairs and belong to “Positive” or “Negative” class. Samples in “Positive” class contain regions wrapping around histone proteins. In contrast, samples in “Negative” class do not contain them. With these datasets, if we could predict histone profiles from sequences with a certain level of accuracy we might help to understand about ex-\n",
    "pression pattern of genes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GTG9m9GxZ_an",
    "outputId": "ea425729-6eb0-4aad-81db-ed0a6df609ff"
   },
   "outputs": [],
   "source": [
    "seqs = []\n",
    "labels = []\n",
    "\n",
    "# every sequence comes as 3 lines. The first line is the name,\n",
    "# the second is the gene\n",
    "# the third is the class\n",
    "\n",
    "f = open('h3.txt')\n",
    "for i, line in enumerate(f):\n",
    "    if i % 3 == 1:\n",
    "        line = re.sub('[^ATCGatcg]', '', line)\n",
    "        seqs.append(line)\n",
    "    if i%3 == 2:\n",
    "        labels.append(int(line[0]))\n",
    "f.close()\n",
    "\n",
    "print(seqs[:5])\n",
    "print(labels[:5])\n",
    "\n",
    "# tokenizing and getting a vocab\n",
    "tokenized_seqs, voc2ind = prepare_data(seqs)\n",
    "\n",
    "# padding\n",
    "tokenized_seqs = pad(tokenized_seqs, voc2ind)\n",
    "\n",
    "# tokenizing labels\n",
    "tokenized_labels, label2token = prepare_labels(labels)\n",
    "\n",
    "# Showing the result of this:\n",
    "print(\"\\n\", tokenized_seqs, \"\\n\\n\", voc2ind, \"\\n\\n\", label_to_count(labels))\n",
    "\n",
    "# Train Test Split\n",
    "train_inputs, test_inputs, train_labels, test_labels = train_test_split(\n",
    "    tokenized_seqs, tokenized_labels, test_size=0.1, random_state=42)\n",
    "\n",
    "# Load data to PyTorch DataLoader\n",
    "train_dataloader, test_dataloader = data_loader(train_inputs, test_inputs, train_labels, test_labels, batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qvCDh25X3y3o"
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4LXoACh31yX"
   },
   "source": [
    "### Simple Model\n",
    "\n",
    "This extremly simple neural network is to sanity check that the data has been processed correctly. Also if an extremly simple neural network with just a relu will classify output, that would also be great."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 725
    },
    "id": "cUYZ_d853yXm",
    "outputId": "1d81c532-518e-4479-8b7b-33c2469d64b4"
   },
   "outputs": [],
   "source": [
    "FEATURE_SIZE = tokenized_seqs.shape[1]\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, vocab_size, feature_size, num_classes=3):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        # UM?\n",
    "        self.vocab_size = vocab_size\n",
    "        # embeded_dim: Dimension of word vectors.\n",
    "        self.feature_size = feature_size\n",
    "\n",
    "        self.encoder = nn.Embedding(self.vocab_size, self.feature_size)\n",
    "\n",
    "        self.fc1 = nn.Linear(feature_size, 1000)\n",
    "        self.fc2 = nn.Linear(1000, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Creating net and training\n",
    "net = SimpleNet(len(voc2ind), FEATURE_SIZE)\n",
    "losses = train(net, train_dataloader, epochs = 10)\n",
    "\n",
    "# Evaluation\n",
    "plot_losses(losses, title = \"H3: Simple Network Loss\")\n",
    "print_eval(net, train_dataloader, test_dataloader)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTEstdzv4-un"
   },
   "source": [
    "### Convolutional Model with 2D Max Pooling\n",
    "Goal to beat: 88.99%\n",
    "\n",
    "This number is not beat.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "v-rJ8Pkz6DE5",
    "outputId": "7cb09e2a-5ff8-4e07-8821-7c6a058ed8d7"
   },
   "outputs": [],
   "source": [
    "# Here are some experiments run with the model\n",
    "# kind of represents a table, each row is an experiment.\n",
    "# the first row is the indecies. Each row should be the same length.\n",
    "ex = [['in_channels_conv1', 'out_channels_conv1', 'in_channels_conv2', 'out_channels_conv2', 'in_channels_fc1', \n",
    "       'kernel_size_conv1', 'stride_conv1', 'kenrnel_size_conv2', 'kernel_size_conv1', \n",
    "       'momentum', 'learning rate', 'decay', ('train_acc'), ('test_acc'),],\n",
    "      [500, 4000, 2000, 8000, 56000, 6, 3, 6, 3,    0.9, 0.01, 0, (\"NaN\"), (\"NaN\")], #1\n",
    "      [500, 1000, 500, 1000, 7000, 6, 3, 6, 3,     0.9,0.01, 0, (0.752895752895753), (0.7441549766199065), 0], #2\n",
    "      [500, 2000, 1000, 4000, 28000, 6, 3, 6, 3,     0.9,0.01, 0, (0.7117612117612118), (0.6913827655310621)], #3\n",
    "      [500, 1000, 500, 1000, 7000, 3, 3, 3, 3,     0.9,0.01, 0, (0.6470151470151471), (0.6479625918503674)], #4\n",
    "      [500, 1000, 500, 1000, 1500, 12, 6, 12, 6,     0.9,0.01, 0, (0.7730917730917731), (0.7234468937875751)], #5\n",
    "      [500, 1000, 500, 1000, 3500, 12, 6, 6, 3,     0.9,0.01, 0, (0.8271458271458272), (0.7682030728122913)], #6\n",
    "      [500, 1500, 750, 1500, 5250, 12, 6, 6, 3,     0.9,0.01, 0, (0.7684882684882685), (0.6947227788911156)], #7\n",
    "      [500, 1000, 500, 1000, 3500, 12, 6, 6, 3,     0.9,0.01, 0.01, (0.8708048708048708), (0.8162992651970608)], #8\n",
    "      [500, 1000, 500, 1000, 3500, 12, 6, 6, 3,     0.9,0.01, 0.001, (0.7771755271755272), (0.7207748830995324)], #9\n",
    "      [500, 1000, 500, 1000, 3500, 12, 6, 6, 3,     0.9,0.01, 0.1, (0.7817047817047817), (0.7762191048764195)], #10\n",
    "      [500, 1000, 500, 1000, 3500, 12, 6, 6, 3,     0.9,0.01, 0.025, (0.8084348084348084), (0.7989311957247829)], #11\n",
    "      [500, 750, 375, 500, 1750, 12, 6, 6, 3,     0.9,0.01, 0.025, (0.7955895455895456), (0.7802271209084837)], #12\n",
    "      [500, 1000, 500, 1000, 3500, 12, 6, 6, 3,     0.9,0.01, 0.0175, (0.7465102465102466), (0.730126920507682)], #13\n",
    "      [500, 1500, 750, 1500, 5250, 12, 6, 6, 3,     0.9,0.01, 0.025, (0.7681170181170182), (0.7615230460921844)], #14\n",
    "      [500, 1000, 500, 1000, 3500, 12, 6, 6, 3,     0.9,0.001, 0.025, (0.8938223938223938), (0.822311289245157)], #15: (This delivers the best test accuracy)\n",
    "      [500, 1000, 500, 1000, 3500, 12, 6, 6, 3,     0.9,0.001, 0.05, (0.7963320463320464), (0.781563126252505)], #16\n",
    "      [500, 1000, 500, 1000, 3500, 12, 6, 6, 3,     0.9,0.001, 0.035, (0.8566231066231066), (0.7975951903807615)], #17\n",
    "      [500, 1000, 500, 1000, 3500, 12, 6, 6, 3,     0.9,0.0001, 0.025, (0.8264775764775765), (0.7909151636606546)], #18 #\n",
    "      [500, 1000, 500, 1000, 3500, 12, 6, 6, 3,     0.9,0.001, 0.025, (0.8693198693198693), (0.8089512358049432)], #19\n",
    "      [500, 1000, 500, 1000, 3500, 12, 6, 6, 3,     0.75,0.001, 0.025, (0.8855806355806356), (0.8203072812291249)], #2\n",
    "      [500, 1000, 500, 1000, 3500, 12, 6, 6, 3,     0.85,0.001, 0.03, (0.876002376002376), (0.8183032732130928)], #21\n",
    "      [500, 1250, 625, 1400, 4900, 12, 6, 6, 3,     0.85,0.001, 0.025, (0.8944163944163944), (0.8156312625250501)], #22\n",
    "      [500, 1000, 500, 1000, 3500, 12, 6, 6, 3,     0.85, 0.001,0.025, (0.888921888921889), (0.8236472945891784)], #23\n",
    "      [500, 1000, 500, 1000, 3500, 12, 6, 6, 3,     0.85,0.001, 0.03, (0.903920403920404), (0.8203072812291249)], #24\n",
    "      [60, 480, 240, 960, 6720, 6, 3, 6, 3,     0.9,0.01, 0, (0.962949212949213), (0.7748830995323981)], #25\n",
    "      [60, 480, 240, 960, 6720, 6, 3, 6, 3,     0.9,0.01, 0.0025, (0.924933174933175), (0.7849031396125584)], #26\n",
    "      [100, 800, 400, 1600, 11200, 6, 3, 6, 3,     0.9,0.01, 0.0025, (0.8551381051381052), (0.7047428189712759)], #27\n",
    "      [60, 240, 120, 240, 1680, 6, 3, 6, 3,     0.9,0.01, 0, (0.8951588951588951), (0.7702070808283233)], #28\n",
    "      [60, 480, 240, 960, 3360, 12, 6, 6, 3,     0.9,0.01, 0, (0.962949212949213), (0.7748830995323981)], #29\n",
    "      [60, 480, 240, 960, 3360, 12, 6, 6, 3,     0.9,0.01, 0.03, (0.8003415503415503), (0.7895791583166333)], #30\n",
    "      [60, 480, 240, 960, 3360, 12, 6, 6, 3,     0.9,0.01, 0.01, (), ()], #31\n",
    "      [60, 480, 240, 960, 3360, 12, 6, 6, 3,     0.9,0.01, 0.02, (0.8152658152658153), (0.800935203740815)], #32\n",
    "      [60, 480, 240, 960, 3360, 12, 6, 6, 3,     0.8,0.01, 0.02, (0.8415503415503416), (0.8096192384769539)], #33\n",
    "      [60, 480, 240, 960, 60000, 3, 1, 3, 1,     0.9,0.01, 0.02, (0.7912087912087912), (0.7688710754843019)], #34\n",
    "      [60, 480, 240, 960, 20160, 6, 3, 3, 1,     0.9,0.01, 0.02, (0.7833382833382834), (0.7561790247160989)], #35\n",
    "      [60, 480, 240, 960, 14880, 6, 2, 6, 2,     0.9,0.01, 0.02, (0.796035046035046), (0.7855711422845691)], #36\n",
    "      [60, 480, 240, 960, 14880, 6, 2, 6, 2,     0.9,0.01, 0.0, (0.5092070092070092), (0.5156980627922512)], #37\n",
    "      [60, 480, 240, 960, 14880, 6, 2, 6, 2,     0.9,0.01, 0.01, (0.7923225423225423), (0.7762191048764195)], #38\n",
    "      [250, 500, 250, 500, 1750, 12, 6, 6, 3,     0.9,0.01, 0.01, (0.7711612711612712), (0.7561790247160989)], #39\n",
    "      [25, 500, 250, 500, 1750, 12, 6, 6, 3,     0.9,0.01, 0.01, (0.7711612711612712), (0.7561790247160989)], #40\n",
    "      [500, 1000, 500, 1000, 3500, 12, 6, 6, 3,     0.9,0.001, 0.03, (0.8853578853578854), (0.81)], #41\n",
    "      [60, 480, 240, 960, 60000, 4, 1, 4, 1,     0.9,0.01, 0.02, (0.805019305019305), (0.7882431529726119)], #42\n",
    "      [25, 100, 50, 200, 12500, 4, 1, 4, 1,     0.9,0.01, 0.02, (0.7852), (0.7649)], #43\n",
    "      [25, 100, 50, 200, 12500, 4, 1, 4, 1,     0.9,0.01, 0.00, (0.87), (0.77)], #44  \n",
    "      [50, 100, 50, 200, 12500, 4, 1, 4, 1,     0.9,0.01, 0.02, (0.69), (0.69)], #45\n",
    "      [500, 1000, 500, 1000, 62500, 4, 1, 4, 1,     0.9,0.001, 0.025, (0.8666), (0.8029)], #46\n",
    "      [60, 480, 240, 960, 60000, 4, 1, 4, 1,     0.9,0.001, 0.025, (0.752), (0.73)], #47\n",
    "      [4, 100, 50, 200, 12500, 4, 1, 4, 1,     0.9,0.001, 0.025, (0.822), (0.8036)], #48\n",
    "      [4, 100, 50, 200, 12500, 4, 1, 4, 1,     0.9,0.001, 0.0, (0.822), (0.8036)], #49\n",
    "      [1, 50, 25, 200, 1400, 6, 3, 6, 3,     0.9,0.001, 0.025, (0.8094), (0.7922)], #50\n",
    "      [2, 100, 50, 400, 2800, 6, 3, 6, 3,     0.9,0.001, 0.025, (0.8094), (0.7922)], #51\n",
    "      [2, 100, 50, 400, 2800, 6, 3, 6, 3,     0.9,0.001, 0.005, (0.8587), (0.8243)], #52 (BEST)\n",
    "      [2, 150, 75, 400, 2800, 6, 3, 6, 3,     0.9,0.001, 0.01, (0.8094), (0.7922)], #53\n",
    "]\n",
    "i = 52\n",
    "\n",
    "FEATURE_SIZE = ex[i][0]\n",
    "\n",
    "class H3_DSC_by_CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, feature_size, num_classes=2):\n",
    "\n",
    "\n",
    "        super(H3_DSC_by_CNN, self).__init__()\n",
    "        # UM?\n",
    "        self.vocab_size = vocab_size\n",
    "        # embeded_dim: Dimension of word vectors.\n",
    "        self.feature_size = feature_size\n",
    "\n",
    "        self.encoder = nn.Embedding(self.vocab_size, self.feature_size)\n",
    "        \n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=ex[i][0],\n",
    "                               out_channels=ex[i][1], \n",
    "                               kernel_size=ex[i][5],\n",
    "                               stride = ex[i][6],\n",
    "                               padding = 1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=ex[i][2],\n",
    "                               out_channels=ex[i][3], \n",
    "                               kernel_size=ex[i][7],\n",
    "                               stride = ex[i][8],\n",
    "                               padding = 1)\n",
    "\n",
    "\n",
    "        # Activation\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "        # Pooling\n",
    "        self.pool = nn.MaxPool2d(3, 2, 1)\n",
    "\n",
    "        # fully connected layer\n",
    "        self.fc1 = nn.Linear(ex[i][4], 100)\n",
    "        self.fc2 = nn.Linear(100, num_classes)\n",
    "\n",
    "        # dropout\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "\n",
    "        # softmax\n",
    "        self.softm = nn.Softmax()\n",
    "\n",
    "\n",
    "    # seen in the network pytorch tutorial\n",
    "    # I've been using this function for years, no idea what it does.\n",
    "    # Would it kill the pytorch tutorial people to document this guy?\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The following line is required to send it through the encoder. Not sure why.\n",
    "        # b_input_ids = torch.tensor(b_input_ids).to(device).long()\n",
    "        x = torch.tensor(x).to(torch.int64)\n",
    "\n",
    "        # Encode. \n",
    "        # Output shape: (b, max_len, embedded_dim)\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # idk flatten it?\n",
    "        # x = torch.flatten(x, 1)\n",
    "\n",
    "        # Permute shuffles dimensions. Do this to satisfy requirments of nn.Conv1d\n",
    "        # Output shape: (b, embedded_dim, max_len)\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        # convolutional layer 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # convolutional layer 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # flattening?\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "\n",
    "        # Tring to figure out the shape\n",
    "        # print(x.shape)\n",
    "\n",
    "        # fully linear layer + dropout\n",
    "        x = self.drop(x)\n",
    "        # x = self.softm(x)\n",
    "        x = self.fc1(x)\n",
    "        # x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Creating net and training\n",
    "net = H3_DSC_by_CNN(len(voc2ind), FEATURE_SIZE)\n",
    "losses = train(net, train_dataloader, epochs = 150, decay = ex[i][-3], lr=ex[i][-4], momentum=ex[i][-5])\n",
    "\n",
    "# Evaluation\n",
    "plot_losses(losses, title = \"H3 Dataset: Convolutional Network Loss\")\n",
    "print_eval(net, train_dataloader, test_dataloader)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "znkONwF95hcq"
   },
   "source": [
    "### Convolutional Model with 1D Max Pooling\n",
    "\n",
    "Goal to beat: 88.99%\n",
    "\n",
    "This number is not beat.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "hBuqXf2xE-xs",
    "outputId": "39c3f3eb-e284-4043-fee5-a461cfd26be6"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Here are some experiments run with the model\n",
    "# kind of represents a table, each row is an experiment.\n",
    "# the first row is the indecies. Each row should be the same length.\n",
    "ex = [['in_channels_conv1', 'out_channels_conv1', 'in_channels_conv2', 'out_channels_conv2', 'in_channels_fc1', \n",
    "       'kernel_size_conv1', 'stride_conv1', 'kenrnel_size_conv2', 'kernel_size_conv1', \n",
    "       'momentum', 'learning rate', 'decay', ('train_acc'), ('test_acc'),],\n",
    "      [60, 480, 480, 960, 6720, 12, 6, 6, 3,     0.8,0.01, 0.02, (0.8193495693495694), (0.7989311957247829)], #1\n",
    "      [60, 240, 240, 960, 6720, 12, 6, 6, 3,     0.8,0.01, 0.02, (0.788016038016038), (0.7768871075484302)], #2\n",
    "      [60, 600, 600, 960, 6720, 12, 6, 6, 3,     0.8,0.01, 0.02, (0.8154885654885655), (0.7935871743486974)], #3\n",
    "      [60, 480, 480, 960, 6720, 12, 6, 6, 3,     0.9,0.01, 0.0, (0.9832194832194833), (0.772879091516366)], #4\n",
    "      [60, 480, 480, 960, 6720, 12, 6, 6, 3,     0.9,0.01, 0.001, (0.9861152361152361), (0.7828991315965264)], #5\n",
    "      [60, 600, 600, 960, 6720, 12, 6, 6, 3,     0.8,0.01, 0.01, (0.8964211464211465), (0.8169672678690715)], #6\n",
    "      [60, 600, 600, 960, 13440, 6, 3, 6, 3,     0.8,0.01, 0.02, (0.809028809028809), (0.7822311289245157)], #7\n",
    "      [60, 960, 960, 1920, 13440, 12, 6, 6, 3,     0.8,0.01, 0.02, (0.8181615681615682), (0.7989311957247829)], #8\n",
    "      [5, 40, 40, 80, 560, 12, 6, 6, 3,     0.9,0.01, 0.0, (0.8316750816750816), (0.7742150968603875)], #9\n",
    "      [10, 80, 80, 160, 1120, 12, 6, 6, 3,     0.9,0.01, 0.0, (0.9221116721116721), (0.7775551102204409)], #10\n",
    "      [10, 80, 80, 160, 1120, 12, 6, 6, 3,     0.9,0.01, 0.001, (0.9102316602316602), (0.7862391449565799)], #11\n",
    "      [5, 40, 40, 80, 1120, 6, 3, 6, 3,     0.9,0.01, 0.0, (0.8594446094446094), (0.8002672010688042)], #12\n",
    "      [5, 20, 20, 40, 560, 6, 3, 6, 3,     0.9,0.01, 0.0, (0.7884615384615384), (0.7535070140280561)], #13\n",
    "      [5, 40, 40, 80, 1120, 6, 3, 6, 3,     0.85,0.01, 0.001, (0.8424413424413424), (0.7929191716766867)], #14\n",
    "      [5, 50, 50, 100, 1400, 6, 3, 6, 3,     0.9,0.01, 0.0, (0.8059103059103059), (0.7528390113560455)], #15\n",
    "      [5, 30, 30, 60, 840, 6, 3, 6, 3,     0.9,0.01, 0.0, (0.8176418176418176), (0.7595190380761523)], #16\n",
    "      [6, 40, 40, 80, 1120, 6, 3, 6, 3,     0.9,0.01, 0.02, (0.8594446094446094), (0.8002672010688042)], #17\n",
    "      [2, 100, 100, 400, 5600, 6, 3, 6, 3,     0.9,0.001, 0.005, (0.8587), (0.8243)], #18\n",
    "]\n",
    "i = 18\n",
    "\n",
    "FEATURE_SIZE = ex[i][0]\n",
    "\n",
    "class H3_DSC_by_CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, feature_size, num_classes=2):\n",
    "\n",
    "\n",
    "        super(H3_DSC_by_CNN, self).__init__()\n",
    "        # UM?\n",
    "        self.vocab_size = vocab_size\n",
    "        # embeded_dim: Dimension of word vectors.\n",
    "        self.feature_size = feature_size\n",
    "\n",
    "        self.encoder = nn.Embedding(self.vocab_size, self.feature_size)\n",
    "        \n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=ex[i][0],\n",
    "                               out_channels=ex[i][1], \n",
    "                               kernel_size=ex[i][5],\n",
    "                               stride = ex[i][6],\n",
    "                               padding = 1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=ex[i][2],\n",
    "                               out_channels=ex[i][3], \n",
    "                               kernel_size=ex[i][7],\n",
    "                               stride = ex[i][8],\n",
    "                               padding = 1)\n",
    "\n",
    "\n",
    "        # Activation\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "        # Pooling\n",
    "        self.pool = nn.MaxPool1d(3, 2, 1)\n",
    "\n",
    "        # fully connected layer\n",
    "        self.fc1 = nn.Linear(ex[i][4], 100)\n",
    "\n",
    "        # dropout\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "\n",
    "        # softmax\n",
    "        self.softm = nn.Softmax()\n",
    "\n",
    "\n",
    "    # seen in the network pytorch tutorial\n",
    "    # I've been using this function for years, no idea what it does.\n",
    "    # Would it kill the pytorch tutorial people to document this guy?\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The following line is required to send it through the encoder. Not sure why.\n",
    "        # b_input_ids = torch.tensor(b_input_ids).to(device).long()\n",
    "        x = torch.tensor(x).to(torch.int64)\n",
    "\n",
    "        # Encode. \n",
    "        # Output shape: (b, max_len, embedded_dim)\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # idk flatten it?\n",
    "        # x = torch.flatten(x, 1)\n",
    "\n",
    "        # Permute shuffles dimensions. Do this to satisfy requirments of nn.Conv1d\n",
    "        # Output shape: (b, embedded_dim, max_len)\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        # convolutional layer 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # convolutional layer 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # flattening?\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "\n",
    "        # Tring to figure out the shape\n",
    "        # print(x.shape)\n",
    "\n",
    "        # fully linear layer + dropout\n",
    "        x = self.drop(x)\n",
    "        # x = self.softm(x)\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Creating net and training\n",
    "net = H3_DSC_by_CNN(len(voc2ind), FEATURE_SIZE)\n",
    "losses = train(net, train_dataloader, epochs = 50, decay = ex[i][-3], lr=ex[i][-4], momentum=ex[i][-5])\n",
    "\n",
    "# Evaluation\n",
    "plot_losses(losses, title = \"H3 Dataset: Convolutional Network Loss\")\n",
    "print_eval(net, train_dataloader, test_dataloader)\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DNA Sequence Classification by CNNs.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
